{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython, TwythonRateLimitError, TwythonError\n",
    "from textblob import TextBlob\n",
    "from glob import glob\n",
    "from csv import DictReader, DictWriter\n",
    "import pandas as pd\n",
    "import os, re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConsumerKey = 'Xap2Jytwc3ehirzhWhPtIg' \n",
    "ConsumerSecret = 'M19t2uyoXSDnNxH01c4EAWvoAScpSyUlBHiGQx4U'\n",
    "AccessToken = '166071132-r247QGhTdb9a1SdtH36avAndjAjwJksdJZLLqDYt'\n",
    "AccessSecret = 'y7CGzzZzwyO72sx7yNedrS3hNXTAPWk5uqjDVdczykaKc'\n",
    "\n",
    "twitter_user = 'HassanNisar'\n",
    "tw = Twython(ConsumerKey, ConsumerSecret, AccessToken, AccessSecret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeline(user_id, max_id=None):    \n",
    "    for i in range(16):\n",
    "        tweets = tw.get_user_timeline(screen_name=user_id,\n",
    "                                      max_id=max_id,\n",
    "                                      count=200,\n",
    "                                      trim_user=True,\n",
    "                                      exclude_replies=True,\n",
    "                                      include_rts=False,\n",
    "                                      tweet_mode='extended')\n",
    "        if len(tweets) > 0:  # last page should have zero results\n",
    "            for tweet in tweets:\n",
    "                max_id = tweet['id'] - 1\n",
    "                yield tweet\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/emallson/9e56a99973b3091124cd\n",
    "# I have a directory of csv files containing user profiles. \n",
    "# Pull the IDs from these.\n",
    "fields = ['id', 'user', 'created_at', 'lang', 'full_text']\n",
    "#fields = ['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
    "#        'entities', 'extended_entities', 'favorite_count', 'favorited',\n",
    "#        'full_text', 'geo', 'id', 'id_str', 'in_reply_to_screen_name',\n",
    "#        'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "#        'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status',\n",
    "#        'lang', 'place', 'possibly_sensitive', 'quoted_status',\n",
    "#        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',\n",
    "#        'retweet_count', 'retweeted', 'source', 'truncated', 'user']\n",
    "\n",
    "users_list = open(\"user.txt\", \"r\")\n",
    "for name in users_list:\n",
    "    path = \"timelines/{}.csv\".format(name.rstrip())\n",
    "    if os.path.exists(path):\n",
    "        continue # skip users that have already been read.\n",
    "    with open(path, \"w\", encoding='utf-8', newline='') as timecsv:\n",
    "        print(name)\n",
    "        writer = DictWriter(timecsv, fields)\n",
    "        writer.writeheader()\n",
    "        # max_id is used to continue a user's timeline from \n",
    "        # the last tweet read in the event that we get \n",
    "        # rate-limited in the middle of a user's timeline\n",
    "        max_id = None\n",
    "        while True:\n",
    "            try:\n",
    "                for tweet in timeline(name, max_id):\n",
    "                    sub = {k: v for k, v in tweet.items()\n",
    "                           if k in fields}\n",
    "                    sub['user'] = sub['user']['id']\n",
    "                    sub['full_text'] = sub['full_text'].rstrip()\n",
    "                    max_id = sub['id']\n",
    "                    if (sub['lang'] == 'ur'):\n",
    "                        writer.writerow(sub)\n",
    "                max_id = None\n",
    "                break\n",
    "            except TwythonRateLimitError as e:\n",
    "                sleep_until(e.retry_after) # sleep until the given date\n",
    "            except TwythonError as e:\n",
    "                # I *think* this is caused by protected profiles. \n",
    "                # I can read some user profile info, but not the timeline\n",
    "                print(e)\n",
    "                print(\"Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document = pd.read_csv('timelines/HassanNisar.csv')\n",
    "\n",
    "df = pd.concat([pd.read_csv(f, encoding='utf-8') for f in glob('timelines/*.csv')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'\\n', '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['full_text'].apply(lambda x: re.sub(r'https?://[A-Za-z0-9./]+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[a-z]','',x,flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('','',string.digits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[،,?!٪\\'“”؟ھ]+\\ *',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [strs for strs in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: give_emoji_free_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "labels = df['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(texts, labels, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer()\n",
    "features_c = cnt_vectorizer.fit_transform(texts)\n",
    "features_c_nd = features.toarray()\n",
    "features_c_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "features = tfidf_vectorizer.fit_transform(texts)\n",
    "features_nd = features.toarray()\n",
    "features_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'تمام اہلیانِ پی ایس ایل کو چاند رات مبارک ہو کل انشاءاللہ سٹیج ب ی سجے گا سیٹی ب ی بجے گی اور دیوانوں کا ک یل ب ی ہوگا # #'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'تمام اہلیانِ پی ایس ایل کو چاند رات مبارک ہو!کل انشاءاللہ سٹیج بھی سجے گا سیٹی بھی بجے گی اور دیوانوں کا کھیل بھی ہوگا!! #KhelDeewanoKa #PSL4'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تمام',\n",
       " 'اہلیانِ',\n",
       " 'پی',\n",
       " 'ایس',\n",
       " 'ایل',\n",
       " 'کو',\n",
       " 'چاند',\n",
       " 'رات',\n",
       " 'مبارک',\n",
       " 'ہو',\n",
       " 'کل',\n",
       " 'انشاءاللہ',\n",
       " 'سٹیج',\n",
       " 'ب',\n",
       " 'ی',\n",
       " 'سجے',\n",
       " 'گا',\n",
       " 'سیٹی',\n",
       " 'ب',\n",
       " 'ی',\n",
       " 'بجے',\n",
       " 'گی',\n",
       " 'اور',\n",
       " 'دیوانوں',\n",
       " 'کا',\n",
       " 'ک',\n",
       " 'یل',\n",
       " 'ب',\n",
       " 'ی',\n",
       " 'ہوگا',\n",
       " '#',\n",
       " '#']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][5].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
